# 借助.Net Aspire快速运行大语言模型

## 背景介绍

作为.NET开发者，应该都十分熟悉`LINQ`，它在语言上极大简化并提升了开发体验。

在微服务开发的背景下，微软推出了`.NET Aspire`，它为开发者提供了使用`C#`来表达和创作`服务编排`的能力，这样在开发中我们可以使用本地的docker来运行多个服务，发布时，可以发布到你使用的云服务(Azure,AWS,Docker compose,Kubernetes)，而不需要编写特定于生产环境的配置文件。

在AI和大语言模型日益强大和易获得的情况下，越来越多的开发者希望能够在本地体验和测试大语言模型，而不是每次都需要调用云服务。

今天我们将介绍如何使用`.Net Aspire` 仅使用几行代码在本地运行大语言模型。

## 前提条件

- .NET SDK 8.0+
- Docker Desktop
- 良好的网络环境
- `Nothing else`

## 通过OpenWebUI访问大语言模型

1. 创建一个新的`.Net Aspire`项目(.NET Aspire App Host)
2. 安装`CommunityToolkit.Aspire.Hosting.Ollama`包
3. 在`Program.cs`中添加以下代码

    ```csharp
    var builder = DistributedApplication.CreateBuilder(args);

    var ollama = builder.AddOllama("ollama")
        .WithDataVolume()
        .WithOpenWebUI()
        .WithGPUSupport()
        .AddModel("phi4:14b");
    
    builder.Build().Run();
    ```

4. 运行项目
5. 打开`NET Aspire Dashboard`，等待所有容器和模型准备完毕。
6. 进入`OpenWebUI`,开始使用。

> [!TIP]
> 你可以到[Ollama官网](https://ollama.com/search)查看可以使用的模型。

> [!CAUTION]
> Aspire 默认要求https，如果遇到https问题，请参考[官方文档](https://learn.microsoft.com/en-us/dotnet/aspire/troubleshooting/allow-unsecure-transport?tabs=windows)，进行配置。
> 如果运行失败，检查镜像是否下载成功，网络是否正常。

### 说明解释

我们使用`Ollama`和`OpenWebUI`来运行大语言模型，在本地，他们将以docker容器的方式运行，所以你无需手动下载和安装他们。

### 代码解释

- `AddOllama`：添加Ollama服务
- `AddModel`：指定了模型名称
- `WithGPUSupport`：默认使用Nvida显卡，可指定参数为AMD
- `WithOpenWebUI` ：提供`Web`管理面板

> [!WARNING]
> OpenWebUI有可能无法正常下载其镜像，我们可以通过调用`ollama`的服务接口来体现其功能。

## 将Ollama集成到自己项目中

`OpenWebUI`提供现成的UI，帮助我们快速体验大语言模型，但是如果它不可用，或者我们想要在自己的项目中调用`Ollama`，就不需要它了，下面来介绍通过代码调用大语言模型。

### 创建一个`ASP.NET Core API`项目

在解决方案中，添加一个`ASP.NET Core API`项目，并集成到`.Net Aspire`项目中。

### 调整Aspire项目代码

修改`Program.cs`文件，添加`API`服务

```csharp
var builder = DistributedApplication.CreateBuilder(args);

var ollama = builder.AddOllama("ollama")
    .WithGPUSupport()
    .WithContainerRuntimeArgs("--gpus=all")
    .WithDataVolume();

var chat = ollama.AddModel("chat", "llama3.2:1b");

builder.AddProject<Projects.ApiService>("apiservice")
    .WithReference(chat)
    .WaitFor(chat);

builder.Build().Run();
```

这里我们使用`llama3.2:1b`模型，并将其命名为**chat**，将将API服务添加到`.NET Aspire`中。

### 在API服务中调用Ollama

先安装`CommunityToolkit.Aspire.OllamaSharp`，这里我们使用最新预览版本。

在API项目的`Program.cs`中，添加以下代码

```csharp
using Microsoft.Extensions.AI;

var builder = WebApplication.CreateBuilder(args);

builder.AddOllamaApiClient("chat")
    .AddChatClient()
    .UseFunctionInvocation()
    .UseOpenTelemetry(configure: t => t.EnableSensitiveData = true)
    .UseLogging();


var app = builder.Build();

app.MapGet("/chat/{question}", async (IChatClient chatClient, string question) =>
{
    var response = await chatClient.GetResponseAsync(question, new ChatOptions
    {
        MaxOutputTokens = 100
    });
    return response.Message;
});

app.Run();

```

现在`.NET Aspire`项目运行起来，然后我们通过`API服务`的接口来测试一下。

## 总结

可以看到，对于.NET开发者来说，想在本地体验大语言模型是非常简单的，只需要几行代码即可，这是其他语言生态难以比拟的优势。

`.NET Aspire`是`.NET`生态独特的解决方案，最初它是为了改善微服务的本地开发体验的，现在，随着开源社区的发展，它可以集成各种各样的服务，比如我们今天使用的`Ollama`。