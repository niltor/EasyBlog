# 借助.Net Aspire快速运行大语言模型

## 背景介绍

作为.NET开发者，应该都十分熟悉`LINQ`，它在语言上极大简化并提升了开发体验。

在微服务开发的背景下，微软推出了`.NET Aspire`，它为开发者提供了使用`C#`来表达和创作`服务编排`的能力，这样在开发中我们可以使用本地的docker来运行多个服务，发布时，可以发布到你使用的云服务(Azure,AWS,Docker compose,Kubernetes)，而不需要编写特定于生产环境的配置文件。

在AI和大语言模型日益强大和易获得的情况下，越来越多的开发者希望能够在本地体验和测试大语言模型，而不是每次都需要调用云服务。

今天我们将介绍如何使用`.Net Aspire` 仅使用几行代码在本地运行大语言模型。

## 前提条件

- .NET SDK 8.0+
- Docker Desktop
- 良好的网络环境
- `Nothing else`

## 开始

1. 创建一个新的`.Net Aspire`项目(.NET Aspire App Host)
2. 安装`CommunityToolkit.Aspire.Hosting.Ollama`包
3. 在`Program.cs`中添加以下代码

    ```csharp
    var builder = DistributedApplication.CreateBuilder(args);

    var ollama = builder.AddOllama("ollama")
        .WithDataVolume()
        .PublishAsContainer()
        .WithOpenWebUI()
        .WithGPUSupport()
        .AddModel("phi4:14b");
    
    builder.Build().Run();
    ```

4. 运行项目
5. 打开`NET Aspire Dashboard`，等待所有容器和模型准备完毕。
6. 进入`OpenWebUI`,开始使用。

> [!TIP]
> 你可以到[Ollama官网](https://ollama.com/search)查看可以使用的模型。

> [!CAUTION]
> Aspire 默认要求https，如果遇到https问题，请参考[官方文档](https://learn.microsoft.com/en-us/dotnet/aspire/troubleshooting/allow-unsecure-transport?tabs=windows)，进行配置。
> 如果运行失败，检查镜像是否下载成功，网络是否正常。

## 说明

我们使用`Ollama`和`OpenWebUI`来运行大语言模型，在本地，他们将以docker容器的方式运行，所以你无需手动下载和安装他们。

可以看到，借助`.NET Aspire`，开发者可以非常优雅的使用代码来定义和运行程序。

### 代码解释

- `AddOllama`：添加Ollama服务
- `AddModel`：指定了模型名称
- `WithGPUSupport`：默认使用Nvida显卡，可指定参数为AMD
- `WithOpenWebUI` ：提供`Web`管理面板

## 总结

可以看到，对于.NET开发者来说，想在本地体验大语言模型是非常简单的，只需要几行代码即可，这是其他语言生态难以比拟的优势。

在本地运行大语言模型，将具有更高的灵活性，随着更多高效的模型，未来我们可以借助本地大语言模型来做更多的事情，如通过微调，让它学习并具有特定技术栈和特定框架的风格的代码能力，以便可以在我们的实际项目中使用。
